{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPZO/fZibF8EEKy+C9gTG4g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SteveChengChen/data-analysis/blob/main/STAT_5243_HW1D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8mdSHwtvT0S",
        "outputId": "73d23029-7b01-46e4-e41f-0e9a6d4f8bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Experiment with N=10000, D=1000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d7951525f95a>:6: RuntimeWarning: overflow encountered in exp\n",
            "  return 1.0 / (1.0 + np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Descent:   iterations=1000, total_time=5.045 s\n",
            "Newton's Method:    iterations=100, total_time=11.635 s\n",
            "Final NLL: GD=89.7977  vs.  Newton=0.0000\n",
            "\n",
            " Experiment with N=1000, D=10 \n",
            "Gradient Descent:   iterations=292, total_time=0.034 s\n",
            "Newton's Method:    iterations=7, total_time=0.002 s\n",
            "Final NLL: GD=385.3359  vs.  Newton=385.3359\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# 1. Basic logistic regression utilities\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def neg_log_likelihood(beta, X, y):\n",
        "\n",
        "    p = sigmoid(X @ beta)\n",
        "\n",
        "    eps = 1e-15\n",
        "    log_likelihood = np.sum( y * np.log(p + eps) + (1 - y)*np.log(1 - p + eps) )\n",
        "    return -log_likelihood\n",
        "\n",
        "def grad_neg_log_likelihood(beta, X, y):\n",
        "\n",
        "    p = sigmoid(X @ beta)\n",
        "    grad = X.T @ (p - y)\n",
        "    return grad\n",
        "\n",
        "def hess_neg_log_likelihood(beta, X, y):\n",
        "\n",
        "    p = sigmoid(X @ beta)\n",
        "\n",
        "    W = p * (1.0 - p)\n",
        "    sqrtW = np.sqrt(W)\n",
        "    X_weighted = X * sqrtW[:, np.newaxis]\n",
        "    H = X_weighted.T @ X_weighted\n",
        "    return H\n",
        "\n",
        "# 2. Gradient Descent and Newton's Method\n",
        "\n",
        "def logistic_regression_gd(X, y, alpha=1e-3, tol=1e-6, max_iter=1000):\n",
        "\n",
        "    N, D = X.shape\n",
        "    beta = np.zeros(D)\n",
        "    obj_history = []\n",
        "\n",
        "    for it in range(max_iter):\n",
        "        grad = grad_neg_log_likelihood(beta, X, y)\n",
        "        beta_new = beta - alpha * grad\n",
        "\n",
        "        if np.linalg.norm(beta_new - beta) < tol:\n",
        "            beta = beta_new\n",
        "            break\n",
        "\n",
        "        beta = beta_new\n",
        "        obj_history.append(neg_log_likelihood(beta, X, y))\n",
        "\n",
        "    return beta, it+1, obj_history\n",
        "\n",
        "def logistic_regression_newton(X, y, tol=1e-9, max_iter=100):\n",
        "\n",
        "    N, D = X.shape\n",
        "    beta = np.zeros(D)\n",
        "    obj_history = []\n",
        "\n",
        "    for it in range(max_iter):\n",
        "        grad = grad_neg_log_likelihood(beta, X, y)\n",
        "        H = hess_neg_log_likelihood(beta, X, y)\n",
        "\n",
        "        H_reg = H + 1e-8 * np.eye(D)\n",
        "        delta = np.linalg.solve(H_reg, -grad)\n",
        "\n",
        "        beta_new = beta + delta\n",
        "        if np.linalg.norm(delta) < tol:\n",
        "            beta = beta_new\n",
        "            break\n",
        "\n",
        "        beta = beta_new\n",
        "        obj_history.append(neg_log_likelihood(beta, X, y))\n",
        "\n",
        "    return beta, it+1, obj_history\n",
        "\n",
        "\n",
        "# 3. Compare two scenarios\n",
        "\n",
        "def generate_synthetic_logistic_data(N, D, seed=42):\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    X = np.random.randn(N, D)\n",
        "    true_beta = np.random.randn(D)\n",
        "    logits = X @ true_beta\n",
        "    probs = sigmoid(logits)\n",
        "    y = (np.random.rand(N) < probs).astype(float)\n",
        "    return X, y\n",
        "\n",
        "def run_experiment(N, D):\n",
        "\n",
        "    print(f\"\\n Experiment with N={N}, D={D} \")\n",
        "    X, y = generate_synthetic_logistic_data(N, D)\n",
        "\n",
        "    start_gd = time.time()\n",
        "    beta_gd, iters_gd, obj_hist_gd = logistic_regression_gd(X, y)\n",
        "    time_gd = time.time() - start_gd\n",
        "\n",
        "    start_newton = time.time()\n",
        "    beta_newton, iters_newton, obj_hist_newton = logistic_regression_newton(X, y)\n",
        "    time_newton = time.time() - start_newton\n",
        "\n",
        "    print(f\"Gradient Descent:   iterations={iters_gd}, total_time={time_gd:.3f} s\")\n",
        "    print(f\"Newton's Method:    iterations={iters_newton}, total_time={time_newton:.3f} s\")\n",
        "\n",
        "    nll_gd = neg_log_likelihood(beta_gd, X, y)\n",
        "    nll_newton = neg_log_likelihood(beta_newton, X, y)\n",
        "    print(f\"Final NLL: GD={nll_gd:.4f}  vs.  Newton={nll_newton:.4f}\")\n",
        "\n",
        "# 4. Main demo\n",
        "if __name__ == \"__main__\":\n",
        "    run_experiment(N=10000, D=1000)\n",
        "\n",
        "    run_experiment(N=1000, D=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Choose a relatively large number of samples N and feature dimension D\n",
        "##### N=10000, D=1000\n",
        "##### Run Gradient Descent(1st approx) versus Newton's Method(2nd approx)\n",
        "##### Gradient Descent took 1000 iterations in 5.045 s\n",
        "##### Newton's Method took 100 iterations in 11.635 s\n",
        "##### the total runtime for 2nd order was slower, suggests that Hessian construction/inversion cost can dominate in large-scale problems\n",
        "##### Choose a smaller problem, N=1000, D=10\n",
        "##### Newton's Method converged in just 7 iterations(about 0.002 s), naive Gradient Descent required 292 iterations (about 0.034 s)\n",
        "##### In smaller, well-conditioned problems, Newton converges in very few steps, outpacing simpler gradient methods\n"
      ],
      "metadata": {
        "id": "gwZ8eunAyPlC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ed-AFpX7vXAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XiHh-MyYvXCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a5f-Dj3RvXE3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}